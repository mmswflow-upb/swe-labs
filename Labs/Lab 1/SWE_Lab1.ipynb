{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# My Software Engineering Lab 1 Notebook: Testing GPT2 on Google Collab"
      ],
      "metadata": {
        "id": "LmUtWWoW1yh9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ummc6_k_TmzS"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "prompt = \"Today I learned how to run a local LLM because\"\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "output = model.generate(**inputs,\n",
        "    max_new_tokens=100,\n",
        "    temperature=0.1,         # creativity: higher = more random\n",
        "    top_k=100,               # limit to top 50 next-token candidates\n",
        "    top_p=0.9,               # nucleus sampling: cumulative prob â‰¤ 0.9\n",
        "    repetition_penalty=1.2,  # discourage repeating phrases\n",
        "    do_sample=True           # enable sampling instead of greedy decoding\n",
        ")\n",
        "\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explaining parameters\n",
        "\n",
        "- tokens: words or phrases are divided into \"tokens\" depending on model and they're decoded back into words or characters using dictionaries provided by the model\n",
        "\n",
        "- max_new_tokens: How many tokens added to the input can it generate.\n",
        "\n",
        "- temperature: lower values (closer to 0) favor peaks in activation functions and makes it select from top tokens (highest probability tokens), while bigger values (closer to 1) increase the pool of tokens that it can select from (smoothes out function), basically it gives chances to less probable tokens to be selected\n",
        "\n",
        "- top_k: samples top k tokens (higher -> more diversity, lower -> more deterministic and more repetitive)\n",
        "\n",
        "- top_p: its an alternative to top_k, it scales up or down so it's not stuck to a number of tokens unlike top_k, this makes it adapt better to different languages since some languages might make heavier usage of tokens (japanese or chinese)\n",
        "\n",
        "- repetition_penalty: values lower than 1 encourage recycling tokens, 1 doesnt affect with anything, and values > 1 discourage and give less weight to already used tokens\n",
        "\n",
        "- do_sample: if off, we use greedy method and we get only the next best token while ignoring temperature, top_k & top_p. Setting this to true enables stochastic sampling.\n",
        "\n"
      ],
      "metadata": {
        "id": "S2B2ztxM49wD"
      }
    }
  ]
}