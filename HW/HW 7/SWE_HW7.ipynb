{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVS6je642F5m"
      },
      "source": [
        "Setting up the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3BUYMPe2CNX"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "import requests\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12jkubbkPxXn"
      },
      "source": [
        "1. Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEDQnihh2RKc",
        "outputId": "d570a9f4-cdc2-45a7-b2ad-43834048c130"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "text length: 99423\n"
          ]
        }
      ],
      "source": [
        "file_path = \"/content/Roman_Empire.txt\"\n",
        "with open(file_path, 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(\"text length:\", len(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jv0n_afe2Qj8"
      },
      "source": [
        "2. Tokenization (character-level)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6_9ITxU2ZMd",
        "outputId": "fe117555-3d11-4b5c-8103-4a2e298e73fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vocab_size: 81 data_len: 99423\n",
            "step 100 | train loss 2.494 | val loss 2.505\n",
            "step 200 | train loss 2.378 | val loss 2.364\n",
            "step 300 | train loss 2.150 | val loss 2.235\n",
            "step 400 | train loss 2.156 | val loss 2.123\n",
            "step 500 | train loss 2.034 | val loss 2.074\n",
            "step 600 | train loss 2.049 | val loss 2.006\n",
            "step 700 | train loss 1.897 | val loss 1.965\n",
            "step 800 | train loss 1.777 | val loss 1.896\n",
            "step 900 | train loss 1.944 | val loss 1.882\n",
            "step 1000 | train loss 1.759 | val loss 1.820\n",
            "step 1100 | train loss 1.714 | val loss 1.815\n",
            "step 1200 | train loss 1.580 | val loss 1.826\n",
            "step 1300 | train loss 1.822 | val loss 1.799\n",
            "step 1400 | train loss 1.701 | val loss 1.759\n",
            "step 1500 | train loss 1.554 | val loss 1.759\n",
            "step 1600 | train loss 1.578 | val loss 1.735\n",
            "step 1700 | train loss 1.646 | val loss 1.715\n",
            "step 1800 | train loss 1.506 | val loss 1.706\n",
            "step 1900 | train loss 1.404 | val loss 1.717\n",
            "step 2000 | train loss 1.464 | val loss 1.697\n",
            "Rome was built becaulthood, and milition tencondenced Christing incipi of the faulic charchitecturality, builk an editings of 144. Leround. Sart mode who hother sold of Asilitary in the postrite persong the tons were mode, antablishe into Rome, which war, into and built of literrange of Law Roman wayed slave coul\n"
          ]
        }
      ],
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "str_to_int = {}\n",
        "for i, ch in enumerate(chars):\n",
        "    str_to_int[ch] = i\n",
        "\n",
        "int_to_str = {}\n",
        "for ch, i in str_to_int.items():\n",
        "    int_to_str[i] = ch\n",
        "\n",
        "\n",
        "def encode(string):\n",
        "    ids = []\n",
        "    for c in string:\n",
        "        ids.append(str_to_int[c])\n",
        "    return ids\n",
        "\n",
        "\n",
        "def decode(ids):\n",
        "    chars = []\n",
        "    for i in ids:\n",
        "        chars.append(int_to_str[i])\n",
        "    return \"\".join(chars)\n",
        "\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(\"vocab_size:\", vocab_size, \"data_len:\", len(data))\n",
        "\n",
        "\n",
        "train_ratio = 0.9\n",
        "split_index = int(train_ratio * len(data))\n",
        "train_data = data[:split_index]\n",
        "val_data = data[split_index:]\n",
        "\n",
        "\n",
        "batch_size = 16\n",
        "block_size = 64\n",
        "\n",
        "\n",
        "def get_batch(split):\n",
        "    if split == \"train\":\n",
        "        d = train_data\n",
        "    else:\n",
        "        d = val_data\n",
        "\n",
        "    max_start = len(d) - block_size - 1\n",
        "    start_positions = torch.randint(0, max_start, (batch_size,))\n",
        "\n",
        "    x_list = []\n",
        "    y_list = []\n",
        "    for start in start_positions:\n",
        "        start = start.item()\n",
        "        x_seq = d[start : start + block_size]\n",
        "        y_seq = d[start + 1 : start + block_size + 1]\n",
        "        x_list.append(x_seq)\n",
        "        y_list.append(y_seq)\n",
        "\n",
        "    x = torch.stack(x_list)\n",
        "    y = torch.stack(y_list)\n",
        "    return x, y\n",
        "\n",
        "\n",
        "def causal_mask(T):\n",
        "    mask = torch.tril(torch.ones(T, T))\n",
        "    return mask.bool()\n",
        "\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        self.q_proj = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k_proj = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v_proj = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        q = self.q_proj(x)\n",
        "        k = self.k_proj(x)\n",
        "        v = self.v_proj(x)\n",
        "\n",
        "        scores = q @ k.transpose(-2, -1)\n",
        "        scores = scores / math.sqrt(C)\n",
        "\n",
        "        mask = causal_mask(T)\n",
        "        scores = scores.masked_fill(~mask, float(\"-inf\"))\n",
        "\n",
        "        weights = F.softmax(scores, dim=-1)\n",
        "\n",
        "        out = weights @ v\n",
        "        return out\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(d_model, 4 * d_model)\n",
        "        self.act = nn.GELU()\n",
        "        self.fc2 = nn.Linear(4 * d_model, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(d_model)\n",
        "        self.attn = SelfAttention(d_model)\n",
        "        self.ln2 = nn.LayerNorm(d_model)\n",
        "        self.ffn = FeedForward(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.ffn(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "d_model = 128\n",
        "n_layers = 4\n",
        "\n",
        "\n",
        "class MiniGPT(nn.Module):\n",
        "    def __init__(self, vocab_size, block_size, d_model, n_layers):\n",
        "        super().__init__()\n",
        "        self.block_size = block_size\n",
        "\n",
        "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_emb = nn.Embedding(block_size, d_model)\n",
        "\n",
        "        blocks = []\n",
        "        for _ in range(n_layers):\n",
        "            blocks.append(DecoderBlock(d_model))\n",
        "        self.blocks = nn.Sequential(*blocks)\n",
        "\n",
        "        self.ln_f = nn.LayerNorm(d_model)\n",
        "        self.lm_head = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        pos = torch.arange(T).unsqueeze(0)\n",
        "\n",
        "        tok = self.token_emb(idx)\n",
        "        pos = self.pos_emb(pos)\n",
        "        x = tok + pos\n",
        "\n",
        "        x = self.blocks(x)\n",
        "\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            logits_2d = logits.view(B * T, vocab_size)\n",
        "            targets_1d = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits_2d, targets_1d)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "\n",
        "model = MiniGPT(vocab_size, block_size, d_model, n_layers)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "max_steps = 2000\n",
        "eval_every = 100\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_on_val():\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    for _ in range(50):\n",
        "        xb, yb = get_batch(\"val\")\n",
        "        _, loss = model(xb, yb)\n",
        "        losses.append(loss.item())\n",
        "    model.train()\n",
        "    return sum(losses) / len(losses)\n",
        "\n",
        "\n",
        "# --- added: loss tracking lists (does not change existing logic) ---\n",
        "steps_log = []\n",
        "train_losses_log = []\n",
        "val_losses_log = []\n",
        "\n",
        "\n",
        "for step in range(1, max_steps + 1):\n",
        "    xb, yb = get_batch(\"train\")\n",
        "    _, loss = model(xb, yb)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if step % eval_every == 0:\n",
        "        val_loss = evaluate_on_val()\n",
        "        train_loss = loss.item()\n",
        "\n",
        "        # --- added: save for plotting ---\n",
        "        steps_log.append(step)\n",
        "        train_losses_log.append(train_loss)\n",
        "        val_losses_log.append(val_loss)\n",
        "\n",
        "        print(f\"step {step} | train loss {train_loss:.3f} | val loss {val_loss:.3f}\")\n",
        "\n",
        "\n",
        "# --- added: plot and save loss curves (does not change existing logic) ---\n",
        "plt.figure()\n",
        "plt.plot(steps_log, train_losses_log, label=\"train loss\")\n",
        "plt.plot(steps_log, val_losses_log, label=\"val loss\")\n",
        "plt.xlabel(\"step\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.title(f\"Loss curves | n_layers={n_layers}, d_model={d_model}, block_size={block_size}\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plot_name = f\"loss_curve_layers{n_layers}_dmodel{d_model}_block{block_size}.png\"\n",
        "plt.savefig(plot_name, dpi=200, bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate(prompt, max_new=200, temperature=1.0):\n",
        "    model.eval()\n",
        "\n",
        "    idx = torch.tensor([encode(prompt)], dtype=torch.long)\n",
        "\n",
        "    for _ in range(max_new):\n",
        "        idx_cond = idx[:, -block_size:]\n",
        "\n",
        "        logits, _ = model(idx_cond)\n",
        "        next_logits = logits[:, -1, :] / temperature\n",
        "\n",
        "        probs = F.softmax(next_logits, dim=-1)\n",
        "        next_id = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "        idx = torch.cat([idx, next_id], dim=1)\n",
        "\n",
        "    return decode(idx[0].tolist())\n",
        "\n",
        "\n",
        "print(generate(\"Rome was built\", max_new=300, temperature=0.9))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
